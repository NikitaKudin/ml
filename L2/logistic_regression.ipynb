{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "%run prepare_for_training.ipynb\n",
    "%run hypothesis.ipynb\n",
    "\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, data, labels, normfalize_data=False):\n",
    "        \"\"\"Logistic regression constructor.\n",
    "        :param data: training set.\n",
    "        :param labels: training set outputs (correct values).\n",
    "        :param normalize_data: flag that indicates that features should be normalized.\n",
    "        \"\"\"\n",
    "\n",
    "        # Normalize features and add ones column.\n",
    "        (\n",
    "            data_processed,\n",
    "            mean,\n",
    "            deviation\n",
    "        ) = prepare_for_training(data, normalize_data)\n",
    "\n",
    "        self.data = data_processed\n",
    "        self.labels = labels\n",
    "        self.unique_labels = np.unique(labels)\n",
    "        self.features_mean = mean\n",
    "        self.features_deviation = deviation\n",
    "        self.normalize_data = normalize_data\n",
    "\n",
    "        # Initialize model parameters.\n",
    "        num_features = self.data.shape[1]\n",
    "        num_unique_labels = np.unique(labels).shape[0]\n",
    "        self.thetas = np.zeros((num_unique_labels, num_features))\n",
    "\n",
    "    def train(self, lambda_param=0, max_iterations=1000):\n",
    "        \"\"\"Trains logistic regression.\n",
    "        :param lambda_param: regularization parameter\n",
    "        :param max_iterations: maximum number of gradient descent iterations.\n",
    "        \"\"\"\n",
    "\n",
    "        # Init cost history array.\n",
    "        cost_histories = []\n",
    "\n",
    "        # Use One-vs-All approach and train the model several times for each label class.\n",
    "\n",
    "        num_features = self.data.shape[1]\n",
    "\n",
    "        # Train the model to distinguish each label particularly.\n",
    "        for label_index, unique_label in enumerate(self.unique_labels):\n",
    "            current_initial_theta = np.copy(self.thetas[label_index]).reshape((num_features, 1))\n",
    "\n",
    "            # Convert labels to array of 0s and 1s for current label class.\n",
    "            current_labels = (self.labels == unique_label).astype(float)\n",
    "\n",
    "            # Run gradient descent.\n",
    "            (current_theta, cost_history) = LogisticRegression.gradient_descent(\n",
    "                self.data,\n",
    "                current_labels,\n",
    "                current_initial_theta,\n",
    "                lambda_param,\n",
    "                max_iterations\n",
    "            )\n",
    "\n",
    "            self.thetas[label_index] = current_theta.T\n",
    "            cost_histories.append(cost_history)\n",
    "\n",
    "        # return self.theta, cost_history\n",
    "        return self.thetas, cost_histories\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"Prediction function\"\"\"\n",
    "\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        data_processed = prepare_for_training(\n",
    "            data,\n",
    "            self.normalize_data\n",
    "        )[0]\n",
    "\n",
    "        probability_predictions = hypothesis(self.thetas.T, data_processed)\n",
    "        max_probability_indices = np.argmax(probability_predictions, axis=1)\n",
    "        class_predictions = np.empty(max_probability_indices.shape, dtype=object)\n",
    "\n",
    "        for index, label in enumerate(self.unique_labels):\n",
    "            class_predictions[max_probability_indices == index] = label\n",
    "\n",
    "        return class_predictions.reshape((num_examples, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_descent(data, labels, initial_theta, lambda_param, max_iteration):\n",
    "        \"\"\"Gradient descent function.\n",
    "        Iteratively optimizes theta model parameters.\n",
    "        :param data: the set of training or test data.\n",
    "        :param labels: training set outputs (0 or 1 that defines the class of an example).\n",
    "        :param initial_theta: initial model parameters.\n",
    "        :param lambda_param: regularization parameter.\n",
    "        :param max_iteration: maximum number of gradient descent steps.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize cost history list.\n",
    "        cost_history = []\n",
    "\n",
    "        # Calculate the number of features.\n",
    "        num_features = data.shape[1]\n",
    "\n",
    "        # Launch gradient descent.\n",
    "        minification_result = minimize(\n",
    "            # Function that we're going to minimize.\n",
    "            lambda current_theta: LogisticRegression.cost_function(\n",
    "                data, labels, current_theta.reshape((num_features, 1)), lambda_param\n",
    "            ),\n",
    "            # Initial values of model parameter.\n",
    "            initial_theta,\n",
    "            # We will use conjugate gradient algorithm.\n",
    "            method='CG',\n",
    "            # Function that will help to calculate gradient direction on each step.\n",
    "            jac=lambda current_theta: LogisticRegression.gradient_step(\n",
    "                data, labels, current_theta.reshape((num_features, 1)), lambda_param\n",
    "            ),\n",
    "            # Record gradient descent progress for debugging.\n",
    "            callback=lambda current_theta: cost_history.append(LogisticRegression.cost_function(\n",
    "                data, labels, current_theta.reshape((num_features, 1)), lambda_param\n",
    "            )),\n",
    "            options={'maxiter': max_iteration}\n",
    "        )\n",
    "\n",
    "        # Throw an error in case if gradient descent ended up with error.\n",
    "        if not minification_result.success:\n",
    "            raise ArithmeticError('Can not minimize cost function: ' + minification_result.message)\n",
    "\n",
    "        # Reshape the final version of model parameters.\n",
    "        optimized_theta = minification_result.x.reshape((num_features, 1))\n",
    "\n",
    "        return optimized_theta, cost_history\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_step(data, labels, theta, lambda_param):\n",
    "        \"\"\"GRADIENT STEP function.\n",
    "        It performs one step of gradient descent for theta parameters.\n",
    "        :param data: the set of training or test data.\n",
    "        :param labels: training set outputs (0 or 1 that defines the class of an example).\n",
    "        :param theta: model parameters.\n",
    "        :param lambda_param: regularization parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize number of training examples.\n",
    "        num_examples = labels.shape[0]\n",
    "\n",
    "        # Calculate hypothesis predictions and difference with labels.\n",
    "        predictions = hypothesis(theta, data)\n",
    "        label_diff = predictions - labels\n",
    "\n",
    "        # Calculate regularization parameter.\n",
    "        regularization_param = (lambda_param / num_examples) * theta\n",
    "\n",
    "        # Calculate gradient steps.\n",
    "        gradients = (1 / num_examples) * (data.T @ label_diff)\n",
    "        regularized_gradients = gradients + regularization_param\n",
    "\n",
    "        # We should NOT regularize the parameter theta_zero.\n",
    "        regularized_gradients[0] = (1 / num_examples) * (data[:, [0]].T @ label_diff)\n",
    "\n",
    "        return regularized_gradients.T.flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def cost_function(data, labels, theta, lambda_param):\n",
    "        \"\"\"Cost function.\n",
    "        It shows how accurate our model is based on current model parameters.\n",
    "        :param data: the set of training or test data.\n",
    "        :param labels: training set outputs (0 or 1 that defines the class of an example).\n",
    "        :param theta: model parameters.\n",
    "        :param lambda_param: regularization parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate the number of training examples and features.\n",
    "        num_examples = data.shape[0]\n",
    "\n",
    "        # Calculate hypothesis.\n",
    "        predictions = hypothesis(theta, data)\n",
    "\n",
    "        # Calculate regularization parameter\n",
    "        # Remember that we should not regularize the parameter theta_zero.\n",
    "        theta_cut = theta[1:, [0]]\n",
    "        reg_param = (lambda_param / (2 * num_examples)) * (theta_cut.T @ theta_cut)\n",
    "\n",
    "        # Calculate current predictions cost.\n",
    "        y_is_set_cost = labels[labels == 1].T @ np.log(predictions[labels == 1])\n",
    "        y_is_not_set_cost = (1 - labels[labels == 0]).T @ np.log(1 - predictions[labels == 0])\n",
    "        cost = (-1 / num_examples) * (y_is_set_cost + y_is_not_set_cost) + reg_param\n",
    "\n",
    "        # Let's extract cost value from the one and only cost numpy matrix cell.\n",
    "        return cost[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
